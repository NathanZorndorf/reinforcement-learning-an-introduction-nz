{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2020 Eduardo Pignatelli\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "W = LinearSegmentedColormap.from_list('w', [\"w\", \"w\"], N=256)\n",
    "\n",
    "ACTIONS = {\n",
    "    0: [1, 0],   # north\n",
    "    1: [-1, 0],  # south\n",
    "    2: [0, -1],  # west\n",
    "    3: [0, 1],   # east\n",
    "}\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        \"\"\"\n",
    "        A gridworld environment with absorbing states at [0, 0] and [size - 1, size - 1].\n",
    "        Args:\n",
    "            size (int): the dimension of the grid in each direction\n",
    "            cell_reward (float): the reward return after extiting any non absorbing state\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.state_value = np.zeros((size, size))\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.state_value = np.zeros((size, size))\n",
    "        return\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # is terminal state?\n",
    "        size = len(self.state_value) - 1\n",
    "        if (state == (0, 0)) or (state == (size, size)):\n",
    "            return state, 0\n",
    "\n",
    "        s_1 = (state[0] + action[0], state[1] + action[1])\n",
    "        reward = -1\n",
    "        # out of bounds north-south\n",
    "        if s_1[0] < 0 or s_1[0] >= len(self.state_value):\n",
    "            s_1 = state\n",
    "        # out of bounds east-west\n",
    "        elif s_1[1] < 0 or s_1[1] >= len(self.state_value):\n",
    "            s_1 = state\n",
    "\n",
    "        return s_1, reward\n",
    "\n",
    "    def render(self, title=None):\n",
    "        \"\"\"\n",
    "        Displays the current value table of mini gridworld environment\n",
    "        \"\"\"\n",
    "        size = len(self.state_value) if len(self.state_value) < 20 else 20\n",
    "        fig, ax = plt.subplots(figsize=(size, size))\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        ax.grid(which='major', axis='both',\n",
    "                linestyle='-', color='k', linewidth=2)\n",
    "        sn.heatmap(self.state_value, annot=True, fmt=\".1f\", cmap=W,\n",
    "                   linewidths=1, linecolor=\"black\", cbar=False)\n",
    "        plt.show()\n",
    "        return fig, ax\n",
    "\n",
    "    def bellman_expectation(self, state, probs, discount):\n",
    "        \"\"\"\n",
    "        Makes a one step lookahead and applies the bellman expectation equation to the state self.state_value[state]\n",
    "        Args:\n",
    "            state (Tuple[int, int]): the x, y indices that define the address on the value table\n",
    "            probs (List[float]): transition probabilities for each action\n",
    "            in_place (bool): if False, the value table is updated after all the new values have been calculated.\n",
    "                             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "        Returns:\n",
    "            (numpy.ndarrray): the new value for the specified state\n",
    "        \"\"\"\n",
    "        # absorbing state\n",
    "        value = 0\n",
    "        for c, action in ACTIONS.items():\n",
    "            s_1, reward = self.step(state, action) # assumes deterministic state dynamics\n",
    "            value += probs[c] * (reward + discount * self.state_value[s_1])\n",
    "        return value\n",
    "\n",
    "    # def bellman_expectation_nz(self, state, probs, discount):\n",
    "        \n",
    "\n",
    "def policy_evaluation(env, policy=None, steps=1, discount=1., in_place=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy (numpy.array): a numpy 3-D numpy array, where the first two dimensions identify a state and the third dimension identifies the actions.\n",
    "                              The array stores the probability of taking each action.\n",
    "        steps (int): the number of iterations of the algorithm\n",
    "        discount (float): discount factor for the bellman equations\n",
    "        in_place (bool): if False, the value table is updated after all the new values have been calculated.\n",
    "             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "    \"\"\"\n",
    "    if policy is None:\n",
    "        # uniform random policy\n",
    "        policy = np.ones((*env.state_value.shape, len(ACTIONS))) * 0.25\n",
    "\n",
    "    for k in range(steps):\n",
    "        # cache old values if not in place\n",
    "        values = env.state_value if in_place else np.empty_like(env.state_value)\n",
    "        for i in range(len(env.state_value)):\n",
    "            for j in range(len(env.state_value[i])):\n",
    "                # apply bellman expectation equation to each state\n",
    "                state = (i, j)\n",
    "                value = env.bellman_expectation(state, policy[i, j], discount)\n",
    "                values[i, j] = value\n",
    "        # set the new value table\n",
    "        env.state_value = values\n",
    "    return env.state_value\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # reprocuce Figure 4.1\n",
    "    for k in [1, 2, 3, 10, 1000]:\n",
    "        env = GridWorld(4)\n",
    "        env.render()\n",
    "        value_table = policy_evaluation(env, steps=k, in_place=False)\n",
    "        env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "20955ba62d2b551f227689328e3939a605cee93f7dcaf3c79672ce0fcf77a2d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
